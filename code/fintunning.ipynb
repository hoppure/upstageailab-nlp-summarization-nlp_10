{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9880a3ac",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62e6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/.pyenv/versions/py12ft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # 모델의 성능을 평가하기 위한 라이브러리입니다.\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71e2c2",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e476d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': {'data_path': '../data/', 'model_name': 'Qwen3:8b', 'output_dir': '../prdiction'}, 'inference': {'batch_size': 10, 'ckt_path': 'model ckt path', 'early_stopping': True, 'generate_max_length': 100, 'no_repeat_ngram_size': 2, 'num_beams': 4, 'remove_tokens': ['<usr>', '<|im_end|>', '<|vision_pad|>'], 'result_path': './prediction/'}, 'tokenizer': {'eos_token': '<|im_end|>', 'max_len': 512, 'special_tokens': ['[SUMM]', '#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']}, 'training': {'do_eval': True, 'do_train': True, 'early_stopping_patience': 3, 'early_stopping_threshold': 0.001, 'evaluation_strategy': 'epoch', 'fp16': True, 'generation_max_length': 100, 'generation_max_new_tokens': 100, 'gradient_accumulation_steps': 1, 'learning_rate': 1e-05, 'load_best_model_at_end': True, 'logging_dir': './logs', 'logging_strategy': 'epoch', 'lr_scheduler_type': 'cosine', 'num_train_epochs': 20, 'optim': 'adamw_torch', 'overwrite_output_dir': True, 'per_device_eval_batch_size': 10, 'per_device_train_batch_size': 10, 'predict_with_generate': True, 'report_to': 'wandb', 'save_strategy': 'epoch', 'save_total_limit': 5, 'seed': 42, 'warmup_ratio': 0.1, 'weight_decay': 0.01}, 'wandb': {'name': 'baseline002', 'project': 'dialogSUM'}}\n"
     ]
    }
   ],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../data/\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.\n",
    "        \"model_name\": \"Qwen3:8b\", #\"digit82/kobart-summarization\", # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.\n",
    "        \"output_dir\": \"../prdiction\" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"max_len\": 512,\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.\n",
    "        \"special_tokens\": [\"[SUMM]\", '#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 20,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 10,\n",
    "        \"per_device_eval_batch_size\": 10,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": 'cosine',\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"evaluation_strategy\": 'epoch',\n",
    "        \"save_strategy\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"seed\": 42,\n",
    "        \"logging_dir\": \"./logs\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 100,\n",
    "        \"generation_max_new_tokens\": 100,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_threshold\": 0.001,\n",
    "        \"report_to\": \"wandb\" # (선택) wandb를 사용할 때 설정합니다.\n",
    "    },\n",
    "    # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.\n",
    "    \"wandb\": {\n",
    "        \"project\": \"dialogSUM\",\n",
    "        \"name\": \"baseline002\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"model ckt path\", # 사전 학습이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 100,\n",
    "        \"num_beams\": 4,\n",
    "        \"batch_size\" : 10,\n",
    "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.\n",
    "        \"remove_tokens\": ['<usr>', \"<|im_end|>\", \"<|vision_pad|>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "config_path = \"./config.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)\n",
    "\n",
    "# 저장된 config 파일을 불러옵니다.\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# 불러온 config 파일의 전체 내용을 확인합니다.\n",
    "print(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4aa476",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0212f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            eos_token: str,\n",
    "        ) -> None:\n",
    "\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    # 실험에 필요한 컬럼을 가져옵니다.\n",
    "    def make_set_as_df(file_path, is_train = True):\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname','dialogue','summary', 'topic']]\n",
    "            return train_df\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname','dialogue']]\n",
    "            return test_df\n",
    "    \n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            # 테스트 시에는 입력 문장만 준비\n",
    "            inputs = []\n",
    "            for idx, row in dataset.iterrows():\n",
    "                input_text = str(row[\"dialogue\"]) + \" [SUMM] \" \n",
    "                inputs.append(input_text)\n",
    "            return inputs\n",
    "            # inputs = dataset['dialogue'].tolist()\n",
    "            # return inputs\n",
    "        else:\n",
    "            inputs = []\n",
    "            labels = []\n",
    "            for idx, row in dataset.iterrows():\n",
    "                # 모델이 요약까지 포함된 전체 문장을 보고 다음 토큰 학습 가능하게 만듬\n",
    "                input_text = str(row[\"dialogue\"]) + \" [SUMM] \" + str(row[\"summary\"]) + \" \" + self.eos_token\n",
    "                inputs.append(input_text)\n",
    "                # 라벨은 입력 시퀀스와 동일하게 하면 causal LM 에서는 shift 라벨링은 트레이너가 처리해서 label prediction 가능\n",
    "                labels.append(input_text)\n",
    "            return inputs, labels\n",
    "    '''\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        if is_test:\n",
    "            # 테스트 시: 디코더 입력이 필요 없다면 빈문자열 또는 None으로 전달\n",
    "            encoder_inputs = dataset['dialogue'].tolist()\n",
    "            decoder_inputs = [\"\"] * len(encoder_inputs)  # 또는 None도 가능\n",
    "            return encoder_inputs, decoder_inputs\n",
    "        else:\n",
    "            encoder_inputs = dataset['dialogue'].tolist()\n",
    "\n",
    "            # BOS 없이 그냥 정답 요약문만 활용\n",
    "            decoder_inputs = [str(summary) for summary in dataset['summary']]\n",
    "            decoder_outputs = [str(summary) + self.eos_token for summary in dataset['summary']]\n",
    "            return encoder_inputs, decoder_inputs, decoder_outputs\n",
    "    '''     \n",
    "        \n",
    "# 2. datasets \n",
    "\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.len = inputs['input_ids'].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.inputs.items():\n",
    "            item[key] = val[idx]\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.len = inputs['input_ids'].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.inputs.items():\n",
    "            item[key] = val[idx]\n",
    "        item['labels'] = self.labels['input_ids'][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, inputs, ids):\n",
    "        self.inputs = inputs\n",
    "        self.ids = ids\n",
    "        self.len = inputs['input_ids'].size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.inputs.items():\n",
    "            item[key] = val[idx]\n",
    "        item['ID'] = self.ids[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf69427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path, 'train.csv')\n",
    "    val_file_path = os.path.join(data_path, 'dev.csv')\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
    "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
    "\n",
    "    # GPT-2용으로 inputs, labels 생성 (make_input은 이전에 GPT-2용으로 수정한 구조 가정)\n",
    "    train_texts, train_labels = preprocessor.make_input(train_data, is_test=False)\n",
    "    val_texts, val_labels = preprocessor.make_input(val_data, is_test=False)\n",
    "\n",
    "    print('-'*10, 'Load data complete', '-'*10)\n",
    "\n",
    "    # tokenizer 호출 (padding, truncation, max_length를 설정)\n",
    "    tokenized_train_inputs = tokenizer(\n",
    "        train_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],  # encoder_max_len == max length로 통일해도 됨\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    tokenized_train_labels = tokenizer(\n",
    "        train_labels,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    tokenized_val_inputs = tokenizer(\n",
    "        val_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    tokenized_val_labels = tokenizer(\n",
    "        val_labels,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # Dataset 객체 생성 (GPT-2용 DatasetForTrain, DatasetForVal 사용)\n",
    "    train_dataset = DatasetForTrain(tokenized_train_inputs, tokenized_train_labels)\n",
    "    val_dataset = DatasetForVal(tokenized_val_inputs, tokenized_val_labels)\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132afab0",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b8cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능에 대한 평가 지표를 정의합니다. 본 대회에서는 ROUGE 점수를 통해 모델의 성능을 평가합니다.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # 정확한 평가를 위해 미리 정의된 불필요한 생성토큰들을 제거합니다.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # 최종적인 ROUGE 점수를 계산합니다.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE 점수 중 F-1 score를 통해 평가합니다.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92b25a",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed7a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(config, model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "\n",
    "    # wandb 초기화\n",
    "    wandb.init(\n",
    "        project=config['wandb']['project'],\n",
    "        name=config['wandb']['name'],\n",
    "    )\n",
    "    # SFTConfig 객체 생성 (TRL 공식 설정 객체)\n",
    "\n",
    "    # 1) SFTConfig 객체 생성\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=\"./output_dir\",\n",
    "        max_length=512,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        # eval_steps=1000,\n",
    "        # save_steps=1,\n",
    "        # save_total_limit=3,\n",
    "        gradient_accumulation_steps=4,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"rougeL\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"wandb\",\n",
    "        save_strategy='steps',\n",
    "        eval_strategy='no',\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        weight_decay = 0.01,\n",
    "\n",
    "    )\n",
    "    '''\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=config['general']['output_dir'],\n",
    "        max_length=config['tokenizer']['max_len'],  # 최대 토큰 길이\n",
    "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
    "        num_train_epochs=config['training']['num_train_epochs'],\n",
    "        learning_rate=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay'],\n",
    "        warmup_ratio=config['training']['warmup_ratio'],\n",
    "        logging_dir=config['training']['logging_dir'],\n",
    "        logging_strategy=config['training']['logging_strategy'],  # ex) 'steps'\n",
    "        logging_steps=config['training'].get('logging_steps', 10),\n",
    "        evaluation_strategy=config['training']['evaluation_strategy'],  # ex) 'steps'\n",
    "        eval_steps=config['training'].get('eval_steps', None),\n",
    "        save_strategy=config['training']['save_strategy'],  # ex) 'steps'\n",
    "        save_steps=config['training'].get('save_steps', None),\n",
    "        save_total_limit=config['training']['save_total_limit'],\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "        fp16=config['training']['fp16'],\n",
    "        seed=config['training']['seed'],\n",
    "        load_best_model_at_end=config['training']['load_best_model_at_end'],\n",
    "        metric_for_best_model=config['training'].get('metric_for_best_model', None),\n",
    "        greater_is_better=config['training'].get('greater_is_better', True),\n",
    "        report_to=config['training']['report_to'],  # e.g. 'wandb' or None\n",
    "    )\n",
    "    '''\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        # eval_dataset=val_inputs_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        # compute_metrics=lambda pred: compute_metrics(pred, tokenizer, config),\n",
    "    )\n",
    "  \n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b95e334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 tokenizer와 사전 학습된 모델을 불러옵니다.\n",
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/qwen3-8B\",\n",
    "    max_seq_length = 512,   # Context length - can be longer, but uses more memory : config['tokenizer']['encoder_max_len']\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    "    )\n",
    "    \n",
    "    special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer)) # 사전에 special token을 추가했으므로 재구성 해줍니다.\n",
    "    model.to(device)\n",
    "\n",
    "    return model , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c6fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'train.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
    "    test_ids = test_data['fname']\n",
    "\n",
    "    inputs = preprocessor.make_input(test_data,is_test=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    # DatasetForInference 생성: 토크나이저 결과 + id 리스트를 함께 넘깁니다.\n",
    "    test_dataset = DatasetForInference(inputs, test_ids)\n",
    "\n",
    "    return test_data, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # 사용할 device를 정의합니다.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # 사용할 모델과 tokenizer를 불러옵니다.\n",
    "    model , tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "    print('-'*10,\"tokenizer special tokens : \",tokenizer.special_tokens_map,'-'*10)\n",
    "\n",
    "    # 학습에 사용할 데이터셋을 불러옵니다.\n",
    "\n",
    "    preprocessor = Preprocess(eos_token=config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str\n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)\n",
    "\n",
    "    # Trainer 클래스를 불러옵니다.\n",
    "    trainer = load_trainer_for_train(config, model,tokenizer,train_inputs_dataset,val_inputs_dataset)\n",
    "    trainer.train()   # 모델 학습을 시작합니다.\n",
    "\n",
    "    model.save_pretrained(\"../output_dir/model\")\n",
    "    tokenizer.save_pretrained(\"../output_dir/tokenizer\")\n",
    "\n",
    "    # (선택) 모델 학습이 완료된 후 wandb를 종료합니다.\n",
    "    wandb.finish()\n",
    "    preprocessor = Preprocess(eos_token=config['tokenizer']['eos_token']) \n",
    "\n",
    "\n",
    "    test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm.tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            generated_ids = model.generate(input_ids=item['input_ids'].to(\"cuda\"),\n",
    "                            attention_mask=item['attention_mask'].to(\"cuda\"),\n",
    "                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                            early_stopping=config['inference']['early_stopping'],\n",
    "                            max_new_tokens=config['inference']['generate_max_length'],\n",
    "                            num_beams=1,#loaded_config['inference']['num_beams'],\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                        )\n",
    "            for ids in generated_ids:\n",
    "                result = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "                summary.append(result)\n",
    "                print(result)\n",
    "                print(\"---\"*50)\n",
    "\n",
    "    # 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data['fname'],\n",
    "            \"summary\" : preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output_qwen.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.7.1+cu126\n",
      "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "Unsloth 2025.7.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- tokenizer special tokens :  {'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['[SUMM]', '#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']} ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "train_data:\n",
      " #Person1#: 안녕하세요, Mr. Smith. 저는 Dr. Hawkins입니다. 오늘 무슨 일로 오셨어요? \n",
      "#Person2#: 건강검진을 받으려고 왔어요. \n",
      "#Person1#: 네, 5년 동안 검진을 안 받으셨네요. 매년 한 번씩 받으셔야 해요. \n",
      "#Person2#: 알죠. 특별히 아픈 데가 없으면 굳이 갈 필요가 없다고 생각했어요. \n",
      "#Person1#: 음, 심각한 질병을 피하려면 미리 발견하는 게 제일 좋거든요. 본인을 위해서라도 매년 한 번은 오세요. \n",
      "#Person2#: 알겠습니다. \n",
      "#Person1#: 여기 좀 볼까요. 눈과 귀는 괜찮으시네요. 깊게 숨 한 번 쉬어보세요. Mr. Smith, 담배 피우세요? \n",
      "#Person2#: 네. \n",
      "#Person1#: 담배가 폐암하고 심장병의 주된 원인인 거 아시죠? 끊으셔야 해요. \n",
      "#Person2#: 수백 번 시도했는데, 도저히 습관이 안 끊어져요. \n",
      "#Person1#: 음, 도움 될만한 수업과 약물들이 있습니다. 가시기 전에 더 정보를 드릴게요. \n",
      "#Person2#: 네, 고맙습니다, 의사 선생님.\n",
      "train_label:\n",
      " Mr. Smith는 Dr. Hawkins에게 건강검진을 받으러 와서, 매년 검진 필요성을 안내받고 흡연 습관 개선을 위한 도움을 제안받았습니다.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "val_data:\n",
      " #Person1#: 안녕하세요, 오늘 기분이 어떠세요?\n",
      "#Person2#: 요즘 숨쉬기가 힘들어요.\n",
      "#Person1#: 최근에 감기에 걸렸나요?\n",
      "#Person2#: 아니요, 감기는 안 걸렸어요. 숨쉴 때 가슴이 답답해요.\n",
      "#Person1#: 혹시 알고 있는 알레르기 있으세요?\n",
      "#Person2#: 아니요, 특별히 알고 있는 알레르기는 없어요.\n",
      "#Person1#: 이게 항상 그런가요, 아니면 주로 활동할 때 그런가요?\n",
      "#Person2#: 운동할 때 특히 많이 그래요.\n",
      "#Person1#: 천식 검사를 위해 폐 전문의에게 가보시는 게 좋겠어요.\n",
      "#Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "val_label:\n",
      " #Person2#는 숨쉬기 어려워합니다. 의사는 #Person2#에게 증상을 확인하고, 천식 검사를 위해 폐 전문의에게 가볼 것을 권합니다.\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n",
      "---------- Make training arguments ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoppure\u001b[0m (\u001b[33mhoppure-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/dev/code/wandb/run-20250727_165045-dqs0xtvt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hoppure-/dialogsum/runs/dqs0xtvt' target=\"_blank\">baseline002</a></strong> to <a href='https://wandb.ai/hoppure-/dialogsum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hoppure-/dialogsum' target=\"_blank\">https://wandb.ai/hoppure-/dialogsum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hoppure-/dialogsum/runs/dqs0xtvt' target=\"_blank\">https://wandb.ai/hoppure-/dialogsum/runs/dqs0xtvt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make trainer complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 12,457 | Num Epochs = 1 | Total steps = 49\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 4 x 1) = 256\n",
      " \"-____-\"     Trainable parameters = 87,293,952 of 8,275,899,392 (1.05% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/49 1:12:35 < 03:13, 0.01 it/s, Epoch 0.94/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>5.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>5.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>5.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>5.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>5.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>4.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>4.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>4.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.796100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87681ac0",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87572697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"unsloth/qwen3-8B\")\n",
    "model.resize_token_embeddings(151676)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, '/data/ephemeral/home/dev/output_dir/model', is_trainable=True)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer_from_directory(tokenizer_dir):\n",
    "    \"\"\"\n",
    "    tokenizer_dir: str\n",
    "        tokenizer가 저장된 디렉토리 경로 (예: 체크포인트 경로)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    return tokenizer\n",
    "tokenizer = load_tokenizer_from_directory(\"/data/ephemeral/home/dev/output_dir/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98de2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12457 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute '_fast_lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(dataloader):\n\u001b[32m     34\u001b[39m     text_ids.extend(item[\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minference\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mno_repeat_ngram_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minference\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mearly_stopping\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minference\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgenerate_max_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#loaded_config['inference']['num_beams'],\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m                    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m generated_ids:\n\u001b[32m     45\u001b[39m         result = tokenizer.decode(ids, skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/peft/peft_model.py:1968\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1967\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1970\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/models/llama.py:1740\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/generation/utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/generation/utils.py:3609\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3607\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3609\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3612\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3613\u001b[39m     outputs,\n\u001b[32m   3614\u001b[39m     model_kwargs,\n\u001b[32m   3615\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3616\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/models/llama.py:1125\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_CausalLM_fast_forward\u001b[39m(\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1109\u001b[39m     input_ids: torch.LongTensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1122\u001b[39m     *args, **kwargs,\n\u001b[32m   1123\u001b[39m ) -> Union[Tuple, CausalLMOutputWithPast]:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m         outputs = \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1133\u001b[39m         causal_mask = xformers.attn_bias.LowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/models/llama.py:1058\u001b[39m, in \u001b[36m_LlamaModel_fast_forward_inference.<locals>.LlamaModel_fast_forward_inference_custom\u001b[39m\u001b[34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[39m\n\u001b[32m   1050\u001b[39m residual.copy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[32m   1051\u001b[39m X = fast_rms_layernorm_inference(\n\u001b[32m   1052\u001b[39m     decoder_layer.input_layernorm,\n\u001b[32m   1053\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1056\u001b[39m     variance = variance,\n\u001b[32m   1057\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m X, present_key_value = \u001b[43mattention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpaged_attention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m X += residual\n\u001b[32m   1068\u001b[39m residual.copy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/models/qwen3.py:282\u001b[39m, in \u001b[36mQwen3Attention_fast_forward_inference\u001b[39m\u001b[34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mself\u001b[39m.attention.resize_((bsz, n_heads, \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.attention.shape[-\u001b[32m1\u001b[39m]+KV_CACHE_INCREMENT))\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m Qn = \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemp_QA\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m Kn = fast_linear_forward(\u001b[38;5;28mself\u001b[39m.k_proj, Xn, out = \u001b[38;5;28mself\u001b[39m.temp_KV[\u001b[32m0\u001b[39m])\n\u001b[32m    284\u001b[39m Vn = fast_linear_forward(\u001b[38;5;28mself\u001b[39m.v_proj, Xn, out = \u001b[38;5;28mself\u001b[39m.temp_KV[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/kernels/utils.py:681\u001b[39m, in \u001b[36mfast_linear_forward\u001b[39m\u001b[34m(proj, X, temp_lora, out)\u001b[39m\n\u001b[32m    679\u001b[39m     out = out.view(out_dim)\n\u001b[32m    680\u001b[39m     temp_lora = torch_mv(lora_A._fast_lora, X.ravel(), out = temp_lora)\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     out.addmv_(\u001b[43mlora_B\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fast_lora\u001b[49m, temp_lora, alpha = lora_S)\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    683\u001b[39m     out = out.view(bsz, out_dim)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Parameter' object has no attribute '_fast_lora'"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocess(eos_token=loaded_config['tokenizer']['eos_token']) \n",
    "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
    "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'train.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
    "    test_ids = test_data['fname']\n",
    "\n",
    "    inputs = preprocessor.make_input(test_data,is_test=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    # DatasetForInference 생성: 토크나이저 결과 + id 리스트를 함께 넘깁니다.\n",
    "    test_dataset = DatasetForInference(inputs, test_ids)\n",
    "\n",
    "    return test_data, test_dataset\n",
    "\n",
    "test_data, test_dataset = prepare_test_dataset(loaded_config, preprocessor, tokenizer)\n",
    "dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "summary = []\n",
    "text_ids = []\n",
    "with torch.no_grad():\n",
    "    for item in tqdm.tqdm(dataloader):\n",
    "        text_ids.extend(item['ID'])\n",
    "        generated_ids = model.generate(input_ids=item['input_ids'].to(\"cuda\"),\n",
    "                        attention_mask=item['attention_mask'].to(\"cuda\"),\n",
    "                        no_repeat_ngram_size=loaded_config['inference']['no_repeat_ngram_size'],\n",
    "                        early_stopping=loaded_config['inference']['early_stopping'],\n",
    "                        max_new_tokens=loaded_config['inference']['generate_max_length'],\n",
    "                        num_beams=1,#loaded_config['inference']['num_beams'],\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "        for ids in generated_ids:\n",
    "            result = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "            summary.append(result)\n",
    "            print(result)\n",
    "            print(\"---\"*50)\n",
    "\n",
    "# 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.\n",
    "remove_tokens = loaded_config['inference']['remove_tokens']\n",
    "preprocessed_summary = summary.copy()\n",
    "for token in remove_tokens:\n",
    "    preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "output = pd.DataFrame(\n",
    "    {\n",
    "        \"fname\": test_data['fname'],\n",
    "        \"summary\" : preprocessed_summary,\n",
    "    }\n",
    ")\n",
    "result_path = loaded_config['inference']['result_path']\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "output.to_csv(os.path.join(result_path, \"output_qwen.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50155e00",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb9deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
    "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
    "    test_ids = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-'*150)\n",
    "\n",
    "    inputs = preprocessor.make_input(test_data,is_test=True)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "     # Qwen3는 causal LM, 디코더-온리 모델이므로 인코더/디코더 구분 X → 입력 토크나이징만 수행\n",
    "    inputs = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config['tokenizer']['max_len'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    # DatasetForInference 생성: 토크나이저 결과 + id 리스트를 함께 넘깁니다.\n",
    "    test_dataset = DatasetForInference(inputs, test_ids)\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10)\n",
    "    return test_data, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01167d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(ckt_path)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(ckt_path)\n",
    "    \n",
    "    # special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\n",
    "    # tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # model.resize_token_embeddings(len(tokenizer)) # 사전에 special token을 추가했으므로 재구성 해줍니다.\n",
    "    model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return model , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델이 생성한 요약문의 출력 결과를 보여줍니다.\n",
    "def inference(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    model , tokenizer = load_tokenizer_and_model_for_test(config, device)\n",
    "\n",
    "    preprocessor = Preprocess(config['tokenizer']['eos_token'])\n",
    "\n",
    "    test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            generated_ids = model.generate(input_ids=item['input_ids'].to(device),\n",
    "                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                            early_stopping=config['inference']['early_stopping'],\n",
    "                            max_length=config['inference']['generate_max_length'],\n",
    "                            num_beams=config['inference']['num_beams'],\n",
    "                        )\n",
    "            for ids in generated_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "\n",
    "    # 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data['fname'],\n",
    "            \"summary\" : preprocessed_summary,\n",
    "        }\n",
    "    )\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ed2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"unsloth/qwen3-8B\")\n",
    "model.resize_token_embeddings(151676)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, '/data/ephemeral/home/dev/output_dir/model', is_trainable=True)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer_from_directory(tokenizer_dir):\n",
    "    \"\"\"\n",
    "    tokenizer_dir: str\n",
    "        tokenizer가 저장된 디렉토리 경로 (예: 체크포인트 경로)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    return tokenizer\n",
    "tokenizer = load_tokenizer_from_directory(\"/data/ephemeral/home/dev/output_dir/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd7d1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer_from_directory(tokenizer_dir):\n",
    "    \"\"\"\n",
    "    tokenizer_dir: str\n",
    "        tokenizer가 저장된 디렉토리 경로 (예: 체크포인트 경로)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    return tokenizer\n",
    "tokenizer = load_tokenizer_from_directory(\"/data/ephemeral/home/dev/output_dir/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.7.1+cu126\n",
      "---------- Load tokenizer & model ----------\n",
      "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151676, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151676, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 학습된 모델의 test를 진행합니다.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output = \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdevice : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m,)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.__version__)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model , tokenizer = \u001b[43mload_tokenizer_and_model_for_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m preprocessor = Preprocess(config[\u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33meos_token\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     11\u001b[39m test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mload_tokenizer_and_model_for_test\u001b[39m\u001b[34m(config, device)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLoad tokenizer & model\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m,)\n\u001b[32m      7\u001b[39m ckt_path = config[\u001b[33m'\u001b[39m\u001b[33minference\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mckt_path\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(ckt_path)\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# tokenizer.add_special_tokens(special_tokens_dict)\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# model.resize_token_embeddings(len(tokenizer)) # 사전에 special token을 추가했으므로 재구성 해줍니다.\u001b[39;00m\n\u001b[32m     16\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/unsloth/models/loader.py:450\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft:\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# From https://github.com/huggingface/peft/issues/184\u001b[39;00m\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# Now add PEFT adapters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mold_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# Patch it as well!\u001b[39;00m\n\u001b[32m    459\u001b[39m     model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/peft/peft_model.py:555\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    547\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    548\u001b[39m         model,\n\u001b[32m    549\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    553\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    567\u001b[39m missing_keys = [\n\u001b[32m    568\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    569\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/peft/peft_model.py:1323\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[32m   1322\u001b[39m ignore_mismatched_sizes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mignore_mismatched_sizes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m load_result = \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m tuner = \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name].peft_type\n\u001b[32m   1332\u001b[39m tuner_prefix = PEFT_TYPE_TO_PREFIX_MAPPING.get(tuner, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/peft/utils/save_and_load.py:455\u001b[39m, in \u001b[36mset_peft_model_state_dict\u001b[39m\u001b[34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    453\u001b[39m             module._move_adapter_to_device_of_base_layer(adapter_name)\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_prompt_learning:\n\u001b[32m    458\u001b[39m     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n\u001b[32m    459\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m: peft_model_state_dict[\u001b[33m\"\u001b[39m\u001b[33mprompt_embeddings\u001b[39m\u001b[33m\"\u001b[39m]}, strict=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    460\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151676, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151676, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096])."
     ]
    }
   ],
   "source": [
    "# 이곳에 내가 사용할 wandb config 설정\n",
    "loaded_config['inference']['ckt_path'] = \"/data/ephemeral/home/dev/code/output_dir/checkpoint-98\"\n",
    "\n",
    "# 학습된 모델의 test를 진행합니다.\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4d354bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "for k, v in list(globals().items()):\n",
    "    # GPU에 있고, 텐서/모델이면\n",
    "    try:\n",
    "        if isinstance(v, torch.Tensor) and v.is_cuda:\n",
    "            del globals()[k]\n",
    "        elif isinstance(v, torch.nn.Module) and v.is_cuda:\n",
    "            del globals()[k]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd0a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "model.embed_tokens.weight doesn't have any device set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      3\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m      4\u001b[39m     llm_int8_enable_fp32_cpu_offload=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m device_map = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformer.word_embeddings\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformer.word_embeddings_layernorm\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformer.ln_f\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     13\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model, tokenizer = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/data/ephemeral/home/dev/code/output_dir/checkpoint-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/modeling_utils.py:5302\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5299\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5302\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5303\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5305\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/modeling_utils.py:933\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12ft/lib/python3.12/site-packages/transformers/modeling_utils.py:819\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    817\u001b[39m module_layer = re.search(device_map_regex, param_name)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_layer:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have any device set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    821\u001b[39m     param_device = device_map[module_layer.group()]\n",
      "\u001b[31mValueError\u001b[39m: model.embed_tokens.weight doesn't have any device set."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": 0,  # 예시: lm_head만 CPU에 올림\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}\n",
    "\n",
    "model, tokenizer = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/ephemeral/home/dev/code/output_dir/checkpoint-3\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device_map,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391ab63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d56204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. PEFT(LoRA) 적용\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# 2. 옵티마이저 설정 (예: AdamW)\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=2e-4)\n",
    "\n",
    "# 3. 데이터로더 정의 (batch_size는 VRAM에 맞게 조절)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1)\n",
    "\n",
    "# 4. SFTTrainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_epochs=3,          # 원하는 학습 epoch 수\n",
    "    gradient_accumulation_steps=8,  # 배치가 너무 작다면 누적 gradient 사용\n",
    "    max_seq_length=2048,   # 입력 최대 길이 (tokenizer 설정과 일치해야 함)\n",
    "    save_dir=\"./peft_finetuned_model\",  # 체크포인트 저장 경로\n",
    "    save_interval=1000,   # step 단위 저장 (필요시 설정)\n",
    "    logging_dir=\"./logs\", # 로그 저장 경로\n",
    "    fp16=True,            # mixed precision 사용 권장 (하드웨어에 따라)\n",
    ")\n",
    "\n",
    "# 5. 학습 실행\n",
    "trainer.fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
